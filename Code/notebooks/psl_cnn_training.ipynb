{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3601cb03-3734-4976-8162-364bcc1915da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os, json, random, glob\n",
    "from pathlib import Path\n",
    "import cv2\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision as tv\n",
    "from torchvision import transforms as T\n",
    "\n",
    "# BASE must be the project root (psl-tutor)\n",
    "BASE = Path(__file__).resolve().parents[1] if \"__file__\" in globals() else Path.cwd().parents[0]\n",
    "\n",
    "RAW = BASE / \"data\" / \"raw\"\n",
    "CROPS = BASE / \"data\" / \"crops\"\n",
    "SPLITS = BASE / \"data\" / \"splits\"\n",
    "ARTF = BASE / \"artifacts\"\n",
    "\n",
    "for p in [CROPS, SPLITS, ARTF]:\n",
    "    p.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "DEVICE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d481f69b-7dc6-47cf-91db-eb8051ee22ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.5.1+cu121 12.1\n",
      "is_available: True\n",
      "NVIDIA GeForce RTX 3070 Ti Laptop GPU\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.__version__, torch.version.cuda)\n",
    "print(\"is_available:\", torch.cuda.is_available())\n",
    "if torch.cuda.is_available():\n",
    "    print(torch.cuda.get_device_name(0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bde3f88d-8bdd-4da0-a4b3-1fed17b38116",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classes: ['Ain', 'Aliph', 'Bari yeh', 'Bay', 'Chay', 'Chhoti yeh', 'Daal', 'Ddaal', 'Dhaal', 'Dhuaad', 'Djay', 'Fay', 'Gaaf', 'Ghain', 'Hamza', 'Hay', 'Jeem', 'Kaaf', 'Khay', 'Laam', 'Meem', 'Noon', 'Pay', 'Quaaf', 'Ray', 'Seen', 'Sheen', 'Suaad', 'Tay', 'Tey', 'Thay', 'Toay_n', 'Vao', 'Zay', 'Zoay_n', 'aRay', 'hey']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'Ain': 195,\n",
       " 'Aliph': 211,\n",
       " 'Bari yeh': 194,\n",
       " 'Bay': 203,\n",
       " 'Chay': 208,\n",
       " 'Chhoti yeh': 200,\n",
       " 'Daal': 203,\n",
       " 'Ddaal': 203,\n",
       " 'Dhaal': 99,\n",
       " 'Dhuaad': 208,\n",
       " 'Djay': 208,\n",
       " 'Fay': 205,\n",
       " 'Gaaf': 209,\n",
       " 'Ghain': 203,\n",
       " 'Hamza': 206,\n",
       " 'Hay': 303,\n",
       " 'Jeem': 205,\n",
       " 'Kaaf': 205,\n",
       " 'Khay': 203,\n",
       " 'Laam': 204,\n",
       " 'Meem': 200,\n",
       " 'Noon': 202,\n",
       " 'Pay': 159,\n",
       " 'Quaaf': 215,\n",
       " 'Ray': 94,\n",
       " 'Seen': 203,\n",
       " 'Sheen': 192,\n",
       " 'Suaad': 203,\n",
       " 'Tay': 210,\n",
       " 'Tey': 204,\n",
       " 'Thay': 186,\n",
       " 'Toay_n': 215,\n",
       " 'Vao': 196,\n",
       " 'Zay': 207,\n",
       " 'Zoay_n': 216,\n",
       " 'aRay': 182,\n",
       " 'hey': 83}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "RAW = (Path.cwd().parents[0] / \"data\" / \"raw\").resolve()\n",
    "\n",
    "classes = sorted([d.name for d in RAW.iterdir() if d.is_dir()])\n",
    "print(\"Classes:\", classes)\n",
    "assert classes, \"No class folders in data/raw\"\n",
    "\n",
    "# Count images per class\n",
    "import glob\n",
    "counts = {c: len(glob.glob(str(RAW/c/\"*.jpg\"))) + \n",
    "             len(glob.glob(str(RAW/c/\"*.png\"))) + \n",
    "             len(glob.glob(str(RAW/c/\"*.jpeg\"))) for c in classes}\n",
    "counts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d7cb4882-034f-481f-841d-bf624c4f3b86",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cropping Ain:   0%|                                                                            | 0/195 [00:00<?, ?it/s]C:\\Users\\syeds\\miniconda3\\envs\\psl-cnn\\lib\\site-packages\\google\\protobuf\\symbol_database.py:55: UserWarning: SymbolDatabase.GetPrototype() is deprecated. Please use message_factory.GetMessageClass() instead. SymbolDatabase.GetPrototype() will be removed soon.\n",
      "  warnings.warn('SymbolDatabase.GetPrototype() is deprecated. Please '\n",
      "Cropping Ain: 100%|██████████████████████████████████████████████████████████████████| 195/195 [00:17<00:00, 10.88it/s]\n",
      "Cropping Aliph: 100%|████████████████████████████████████████████████████████████████| 211/211 [00:19<00:00, 10.69it/s]\n",
      "Cropping Bari yeh: 100%|█████████████████████████████████████████████████████████████| 194/194 [00:18<00:00, 10.68it/s]\n",
      "Cropping Bay: 100%|██████████████████████████████████████████████████████████████████| 203/203 [00:18<00:00, 10.78it/s]\n",
      "Cropping Chay: 100%|█████████████████████████████████████████████████████████████████| 208/208 [00:19<00:00, 10.86it/s]\n",
      "Cropping Chhoti yeh: 100%|███████████████████████████████████████████████████████████| 200/200 [00:18<00:00, 10.78it/s]\n",
      "Cropping Daal: 100%|█████████████████████████████████████████████████████████████████| 203/203 [00:18<00:00, 11.03it/s]\n",
      "Cropping Ddaal: 100%|████████████████████████████████████████████████████████████████| 203/203 [00:18<00:00, 10.77it/s]\n",
      "Cropping Dhaal: 100%|██████████████████████████████████████████████████████████████████| 99/99 [00:08<00:00, 11.43it/s]\n",
      "Cropping Dhuaad: 100%|███████████████████████████████████████████████████████████████| 208/208 [00:19<00:00, 10.75it/s]\n",
      "Cropping Djay: 100%|█████████████████████████████████████████████████████████████████| 208/208 [00:19<00:00, 10.86it/s]\n",
      "Cropping Fay: 100%|██████████████████████████████████████████████████████████████████| 205/205 [00:18<00:00, 10.86it/s]\n",
      "Cropping Gaaf: 100%|█████████████████████████████████████████████████████████████████| 209/209 [00:19<00:00, 10.69it/s]\n",
      "Cropping Ghain: 100%|████████████████████████████████████████████████████████████████| 203/203 [00:16<00:00, 12.43it/s]\n",
      "Cropping Hamza: 100%|████████████████████████████████████████████████████████████████| 206/206 [00:19<00:00, 10.76it/s]\n",
      "Cropping Hay: 100%|██████████████████████████████████████████████████████████████████| 303/303 [00:26<00:00, 11.33it/s]\n",
      "Cropping Jeem: 100%|█████████████████████████████████████████████████████████████████| 205/205 [00:18<00:00, 10.83it/s]\n",
      "Cropping Kaaf: 100%|█████████████████████████████████████████████████████████████████| 205/205 [00:18<00:00, 10.97it/s]\n",
      "Cropping Khay: 100%|█████████████████████████████████████████████████████████████████| 203/203 [00:18<00:00, 11.09it/s]\n",
      "Cropping Laam: 100%|█████████████████████████████████████████████████████████████████| 204/204 [00:18<00:00, 10.97it/s]\n",
      "Cropping Meem: 100%|█████████████████████████████████████████████████████████████████| 200/200 [00:17<00:00, 11.28it/s]\n",
      "Cropping Noon: 100%|█████████████████████████████████████████████████████████████████| 202/202 [00:16<00:00, 12.54it/s]\n",
      "Cropping Pay: 100%|██████████████████████████████████████████████████████████████████| 159/159 [00:14<00:00, 11.02it/s]\n",
      "Cropping Quaaf: 100%|████████████████████████████████████████████████████████████████| 215/215 [00:19<00:00, 11.05it/s]\n",
      "Cropping Ray: 100%|████████████████████████████████████████████████████████████████████| 94/94 [00:08<00:00, 10.89it/s]\n",
      "Cropping Seen: 100%|█████████████████████████████████████████████████████████████████| 203/203 [00:15<00:00, 13.35it/s]\n",
      "Cropping Sheen: 100%|████████████████████████████████████████████████████████████████| 192/192 [00:16<00:00, 11.57it/s]\n",
      "Cropping Suaad: 100%|████████████████████████████████████████████████████████████████| 203/203 [00:18<00:00, 10.72it/s]\n",
      "Cropping Tay: 100%|██████████████████████████████████████████████████████████████████| 210/210 [00:16<00:00, 12.55it/s]\n",
      "Cropping Tey: 100%|██████████████████████████████████████████████████████████████████| 204/204 [00:18<00:00, 10.83it/s]\n",
      "Cropping Thay: 100%|█████████████████████████████████████████████████████████████████| 186/186 [00:17<00:00, 10.72it/s]\n",
      "Cropping Toay_n: 100%|███████████████████████████████████████████████████████████████| 215/215 [00:19<00:00, 10.76it/s]\n",
      "Cropping Vao: 100%|██████████████████████████████████████████████████████████████████| 196/196 [00:18<00:00, 10.77it/s]\n",
      "Cropping Zay: 100%|██████████████████████████████████████████████████████████████████| 207/207 [00:19<00:00, 10.74it/s]\n",
      "Cropping Zoay_n: 100%|███████████████████████████████████████████████████████████████| 216/216 [00:19<00:00, 10.92it/s]\n",
      "Cropping aRay: 100%|█████████████████████████████████████████████████████████████████| 182/182 [00:16<00:00, 11.21it/s]\n",
      "Cropping hey: 100%|████████████████████████████████████████████████████████████████████| 83/83 [00:07<00:00, 10.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done. Crops saved to: C:\\Users\\syeds\\Documents\\psl-tutor\\data\\crops\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import glob, cv2\n",
    "import mediapipe as mp\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "\n",
    "BASE   = Path.cwd().parents[0]\n",
    "RAW    = BASE / \"data\" / \"raw\"\n",
    "CROPS  = BASE / \"data\" / \"crops\"\n",
    "CROPS.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "mp_hands = mp.solutions.hands.Hands(static_image_mode=True, max_num_hands=1)\n",
    "\n",
    "def expand_box(xmin, ymin, xmax, ymax, w, h, pad=0.25):\n",
    "    cx = (xmin + xmax) / 2.0\n",
    "    cy = (ymin + ymax) / 2.0\n",
    "    side = max(xmax - xmin, ymax - ymin)\n",
    "    side = int(side * (1.0 + pad))\n",
    "    x0 = max(0, int(cx - side // 2)); y0 = max(0, int(cy - side // 2))\n",
    "    x1 = min(w, x0 + side);          y1 = min(h, y0 + side)\n",
    "    return x0, y0, x1, y1\n",
    "\n",
    "classes = sorted([d.name for d in RAW.iterdir() if d.is_dir()])\n",
    "for cls in classes:\n",
    "    in_dir  = RAW / cls\n",
    "    out_dir = CROPS / cls\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "    files = (glob.glob(str(in_dir/\"*.jpg\")) + \n",
    "             glob.glob(str(in_dir/\"*.png\")) + \n",
    "             glob.glob(str(in_dir/\"*.jpeg\")))\n",
    "    for p in tqdm(files, desc=f\"Cropping {cls}\"):\n",
    "        img = cv2.imread(p)\n",
    "        if img is None: \n",
    "            continue\n",
    "        h, w = img.shape[:2]\n",
    "        res = mp_hands.process(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))\n",
    "        if res.multi_hand_landmarks:\n",
    "            lm = res.multi_hand_landmarks[0].landmark\n",
    "            xs = [int(l.x * w) for l in lm]; ys = [int(l.y * h) for l in lm]\n",
    "            x0, y0, x1, y1 = expand_box(min(xs), min(ys), max(xs), max(ys), w, h, pad=0.25)\n",
    "            crop = img[y0:y1, x0:x1]\n",
    "        else:\n",
    "            # Fallback: center square crop\n",
    "            side = min(w, h)\n",
    "            x0 = (w - side)//2; y0 = (h - side)//2\n",
    "            crop = img[y0:y0+side, x0:x0+side]\n",
    "        crop = cv2.resize(crop, (224, 224), interpolation=cv2.INTER_LINEAR)\n",
    "        cv2.imwrite(str(out_dir / Path(p).name), crop)\n",
    "\n",
    "print(\"Done. Crops saved to:\", CROPS)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4509f053-ecfb-4bcd-a0c0-e0ba47e98e1a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5056,\n",
       " 1070,\n",
       " 1116,\n",
       " {'Ain': 0,\n",
       "  'Aliph': 1,\n",
       "  'Bari yeh': 2,\n",
       "  'Bay': 3,\n",
       "  'Chay': 4,\n",
       "  'Chhoti yeh': 5,\n",
       "  'Daal': 6,\n",
       "  'Ddaal': 7,\n",
       "  'Dhaal': 8,\n",
       "  'Dhuaad': 9,\n",
       "  'Djay': 10,\n",
       "  'Fay': 11,\n",
       "  'Gaaf': 12,\n",
       "  'Ghain': 13,\n",
       "  'Hamza': 14,\n",
       "  'Hay': 15,\n",
       "  'Jeem': 16,\n",
       "  'Kaaf': 17,\n",
       "  'Khay': 18,\n",
       "  'Laam': 19,\n",
       "  'Meem': 20,\n",
       "  'Noon': 21,\n",
       "  'Pay': 22,\n",
       "  'Quaaf': 23,\n",
       "  'Ray': 24,\n",
       "  'Seen': 25,\n",
       "  'Sheen': 26,\n",
       "  'Suaad': 27,\n",
       "  'Tay': 28,\n",
       "  'Tey': 29,\n",
       "  'Thay': 30,\n",
       "  'Toay_n': 31,\n",
       "  'Vao': 32,\n",
       "  'Zay': 33,\n",
       "  'Zoay_n': 34,\n",
       "  'aRay': 35,\n",
       "  'hey': 36})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json, random, glob\n",
    "from pathlib import Path\n",
    "\n",
    "random.seed(42)\n",
    "BASE   = Path.cwd().parents[0]\n",
    "CROPS  = BASE / \"data\" / \"crops\"\n",
    "SPLITS = BASE / \"data\" / \"splits\"\n",
    "ARTF   = BASE / \"artifacts\"\n",
    "SPLITS.mkdir(parents=True, exist_ok=True)\n",
    "ARTF.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "classes = sorted([d.name for d in CROPS.iterdir() if d.is_dir()])\n",
    "class_to_idx = {c:i for i,c in enumerate(classes)}\n",
    "json.dump(class_to_idx, open(ARTF/\"classes.json\",\"w\"), indent=2)\n",
    "\n",
    "def list_images(d):\n",
    "    return (glob.glob(str(d/\"*.jpg\")) + \n",
    "            glob.glob(str(d/\"*.png\")) + \n",
    "            glob.glob(str(d/\"*.jpeg\")))\n",
    "\n",
    "train, val, test = [], [], []\n",
    "for c in classes:\n",
    "    files = list_images(CROPS / c)\n",
    "    random.shuffle(files)\n",
    "    n = len(files)\n",
    "    n_train = int(0.7*n); n_val = int(0.15*n)\n",
    "    train += [(p, c) for p in files[:n_train]]\n",
    "    val   += [(p, c) for p in files[n_train:n_train+n_val]]\n",
    "    test  += [(p, c) for p in files[n_train+n_val:]]\n",
    "\n",
    "def write_split(lst, name):\n",
    "    with open(SPLITS / name, \"w\", encoding=\"utf-8\") as f:\n",
    "        for p, c in lst:\n",
    "            f.write(f\"{p}\\t{c}\\n\")\n",
    "\n",
    "write_split(train, \"train.txt\")\n",
    "write_split(val,   \"val.txt\")\n",
    "write_split(test,  \"test.txt\")\n",
    "\n",
    "len(train), len(val), len(test), class_to_idx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6cd8d42e-aad6-4882-9f9d-45fe28b95a01",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms as T\n",
    "from PIL import Image\n",
    "\n",
    "IMSIZE = 224\n",
    "MEAN = [0.485, 0.456, 0.406]\n",
    "STD  = [0.229, 0.224, 0.225]\n",
    "\n",
    "class FileListDataset(Dataset):\n",
    "    def __init__(self, list_path, class_map_path, train=True):\n",
    "        self.items = []\n",
    "        with open(list_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            for line in f:\n",
    "                p, lbl = line.strip().split(\"\\t\")\n",
    "                self.items.append((p, lbl))\n",
    "        with open(class_map_path, \"r\") as f:\n",
    "            self.class_to_idx = json.load(f)\n",
    "\n",
    "        if train:\n",
    "            self.tf = T.Compose([\n",
    "                T.Resize((IMSIZE, IMSIZE)),\n",
    "                T.ColorJitter(0.2,0.2,0.2,0.1),\n",
    "                T.RandomRotation(15),\n",
    "                T.RandomPerspective(0.1, p=0.3),\n",
    "                T.ToTensor(),\n",
    "                T.Normalize(MEAN, STD),\n",
    "            ])\n",
    "        else:\n",
    "            self.tf = T.Compose([\n",
    "                T.Resize((IMSIZE, IMSIZE)),\n",
    "                T.ToTensor(),\n",
    "                T.Normalize(MEAN, STD),\n",
    "            ])\n",
    "\n",
    "    def __len__(self): return len(self.items)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        p, lbl = self.items[i]\n",
    "        x = Image.open(p).convert(\"RGB\")\n",
    "        x = self.tf(x)\n",
    "        y = self.class_to_idx[lbl]\n",
    "        return x, y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4b2a1a0a-a0f2-465a-9ae4-f786919174ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5056, 1070)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from pathlib import Path\n",
    "\n",
    "BASE   = Path.cwd().parents[0]\n",
    "SPLITS = BASE / \"data\" / \"splits\"\n",
    "ARTF   = BASE / \"artifacts\"\n",
    "\n",
    "batch_size = 64\n",
    "\n",
    "train_ds = FileListDataset(SPLITS/\"train.txt\", ARTF/\"classes.json\", train=True)\n",
    "val_ds   = FileListDataset(SPLITS/\"val.txt\",   ARTF/\"classes.json\", train=False)\n",
    "\n",
    "# Windows/Jupyter safe: no worker processes\n",
    "train_dl = DataLoader(\n",
    "    train_ds, batch_size=batch_size, shuffle=True,\n",
    "    num_workers=0, pin_memory=True, persistent_workers=False\n",
    ")\n",
    "val_dl = DataLoader(\n",
    "    val_ds, batch_size=batch_size, shuffle=False,\n",
    "    num_workers=0, pin_memory=True, persistent_workers=False\n",
    ")\n",
    "\n",
    "len(train_ds), len(val_ds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7abdc7ec-51a8-4fbc-a5dd-917e23dba150",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([64, 3, 224, 224]),\n",
       " torch.Size([64]),\n",
       " tensor([23, 17, 20, 15, 10, 25, 14, 34, 11, 28]))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Must return quickly with shapes; if it hangs here, paths/images are the issue.\n",
    "xb, yb = next(iter(train_dl))\n",
    "xb.shape, yb.shape, yb[:10]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7cf95781-46e6-4137-a18a-b43cf9667145",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/mobilenet_v3_small-047dcff4.pth\" to C:\\Users\\syeds/.cache\\torch\\hub\\checkpoints\\mobilenet_v3_small-047dcff4.pth\n",
      "100%|█████████████████████████████████████████████████████████████████████████████| 9.83M/9.83M [00:04<00:00, 2.40MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 5, loss=3.4415\n",
      "batch 10, loss=3.0582\n",
      "batch 15, loss=2.5521\n",
      "batch 20, loss=2.0337\n",
      "time for 20 batches: 13.79 s\n"
     ]
    }
   ],
   "source": [
    "import time, json, torch, torchvision as tv, torch.nn as nn, torch.optim as optim\n",
    "torch.backends.cudnn.benchmark = True\n",
    "\n",
    "num_classes = len(json.load(open(ARTF/\"classes.json\")))\n",
    "model = tv.models.mobilenet_v3_small(weights=\"IMAGENET1K_V1\")\n",
    "model.classifier[3] = nn.Linear(model.classifier[3].in_features, num_classes)\n",
    "model = model.cuda()\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.AdamW(model.parameters(), lr=3e-4, weight_decay=1e-4)\n",
    "scaler = torch.amp.GradScaler(\"cuda\")\n",
    "\n",
    "model.train()\n",
    "t0 = time.time()\n",
    "for bi, (xb, yb) in enumerate(train_dl, start=1):\n",
    "    xb = xb.cuda(non_blocking=True); yb = yb.cuda(non_blocking=True)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    with torch.autocast(device_type=\"cuda\", dtype=torch.float16):\n",
    "        logits = model(xb); loss = criterion(logits, yb)\n",
    "    scaler.scale(loss).backward()\n",
    "    scaler.step(optimizer); scaler.update()\n",
    "    if bi % 5 == 0:\n",
    "        print(f\"batch {bi}, loss={loss.item():.4f}\")\n",
    "    if bi >= 20:  # run 20 batches to confirm it moves\n",
    "        break\n",
    "print(\"time for 20 batches:\", round(time.time()-t0,2), \"s\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f6feb413-e6c1-4068-b3c1-c46f3bf45c6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val acc: 0.7963\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val acc: 0.9972\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val acc: 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val acc: 0.9991\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val acc: 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val acc: 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val acc: 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val acc: 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val acc: 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val acc: 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val acc: 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val acc: 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val acc: 0.9991\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val acc: 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val acc: 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val acc: 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val acc: 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val acc: 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val acc: 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val acc: 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val acc: 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val acc: 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val acc: 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val acc: 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val acc: 1.0000\n",
      "Best val acc: 1.0 Saved: C:\\Users\\syeds\\Documents\\psl-tutor\\artifacts\\psl_mnv3_best.pth\n"
     ]
    }
   ],
   "source": [
    "import json, torch, torchvision as tv, torch.nn as nn, torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "\n",
    "num_classes = len(json.load(open(ARTF/\"classes.json\")))\n",
    "model = tv.models.mobilenet_v3_small(weights=\"IMAGENET1K_V1\")\n",
    "model.classifier[3] = nn.Linear(model.classifier[3].in_features, num_classes)\n",
    "model = model.cuda()\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.AdamW(model.parameters(), lr=3e-4, weight_decay=1e-4)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=25)\n",
    "scaler = torch.amp.GradScaler(\"cuda\")\n",
    "torch.backends.cudnn.benchmark = True\n",
    "\n",
    "best_acc, best_path = 0.0, ARTF / \"psl_mnv3_best.pth\"\n",
    "epochs = 25\n",
    "\n",
    "for ep in range(1, epochs+1):\n",
    "    model.train()\n",
    "    for xb, yb in tqdm(train_dl, desc=f\"Epoch {ep}/{epochs}\", leave=False):\n",
    "        xb = xb.cuda(non_blocking=True); yb = yb.cuda(non_blocking=True)\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        with torch.autocast(device_type=\"cuda\", dtype=torch.float16):\n",
    "            logits = model(xb); loss = criterion(logits, yb)\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer); scaler.update()\n",
    "\n",
    "    # validation\n",
    "    model.eval(); correct = n = 0\n",
    "    with torch.no_grad(), torch.autocast(device_type=\"cuda\", dtype=torch.float16):\n",
    "        for xb, yb in val_dl:\n",
    "            xb = xb.cuda(non_blocking=True); yb = yb.cuda(non_blocking=True)\n",
    "            pred = model(xb).argmax(1)\n",
    "            correct += (pred==yb).sum().item(); n += yb.size(0)\n",
    "    acc = correct / max(1, n)\n",
    "    scheduler.step()\n",
    "    print(f\"Val acc: {acc:.4f}\")\n",
    "\n",
    "    if acc > best_acc:\n",
    "        best_acc = acc\n",
    "        torch.save({\"state_dict\": model.state_dict(),\n",
    "                    \"classes\": json.load(open(ARTF/\"classes.json\"))},\n",
    "                   best_path)\n",
    "\n",
    "print(\"Best val acc:\", best_acc, \"Saved:\", best_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bfd55030-e215-4ac8-a7aa-0435f005d9e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\syeds\\AppData\\Local\\Temp\\ipykernel_14220\\2286020566.py:11: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  ckpt = torch.load(ARTF/\"psl_mnv3_best.pth\", map_location=\"cpu\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         Ain       1.00      1.00      1.00        30\n",
      "       Aliph       1.00      1.00      1.00        33\n",
      "    Bari yeh       1.00      1.00      1.00        30\n",
      "         Bay       1.00      1.00      1.00        31\n",
      "        Chay       1.00      1.00      1.00        32\n",
      "  Chhoti yeh       1.00      1.00      1.00        30\n",
      "        Daal       1.00      1.00      1.00        31\n",
      "       Ddaal       1.00      1.00      1.00        31\n",
      "       Dhaal       1.00      1.00      1.00        16\n",
      "      Dhuaad       1.00      1.00      1.00        32\n",
      "        Djay       1.00      1.00      1.00        32\n",
      "         Fay       1.00      1.00      1.00        32\n",
      "        Gaaf       1.00      1.00      1.00        32\n",
      "       Ghain       1.00      1.00      1.00        31\n",
      "       Hamza       1.00      1.00      1.00        32\n",
      "         Hay       1.00      1.00      1.00        46\n",
      "        Jeem       1.00      1.00      1.00        32\n",
      "        Kaaf       1.00      1.00      1.00        32\n",
      "        Khay       1.00      1.00      1.00        31\n",
      "        Laam       1.00      1.00      1.00        32\n",
      "        Meem       1.00      1.00      1.00        30\n",
      "        Noon       1.00      1.00      1.00        31\n",
      "         Pay       1.00      1.00      1.00        25\n",
      "       Quaaf       1.00      1.00      1.00        33\n",
      "         Ray       1.00      1.00      1.00        15\n",
      "        Seen       1.00      1.00      1.00        31\n",
      "       Sheen       1.00      1.00      1.00        30\n",
      "       Suaad       1.00      1.00      1.00        31\n",
      "         Tay       1.00      1.00      1.00        32\n",
      "         Tey       1.00      1.00      1.00        32\n",
      "        Thay       1.00      1.00      1.00        29\n",
      "      Toay_n       1.00      1.00      1.00        33\n",
      "         Vao       1.00      1.00      1.00        30\n",
      "         Zay       1.00      1.00      1.00        32\n",
      "      Zoay_n       1.00      1.00      1.00        33\n",
      "        aRay       1.00      1.00      1.00        28\n",
      "         hey       1.00      1.00      1.00        13\n",
      "\n",
      "    accuracy                           1.00      1116\n",
      "   macro avg       1.00      1.00      1.00      1116\n",
      "weighted avg       1.00      1.00      1.00      1116\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[30,  0,  0, ...,  0,  0,  0],\n",
       "       [ 0, 33,  0, ...,  0,  0,  0],\n",
       "       [ 0,  0, 30, ...,  0,  0,  0],\n",
       "       ...,\n",
       "       [ 0,  0,  0, ..., 33,  0,  0],\n",
       "       [ 0,  0,  0, ...,  0, 28,  0],\n",
       "       [ 0,  0,  0, ...,  0,  0, 13]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# H) Evaluate best model on the test set\n",
    "import json, torch, torchvision as tv, torch.nn as nn\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from pathlib import Path\n",
    "\n",
    "BASE   = Path.cwd().parents[0]\n",
    "SPLITS = BASE / \"data\" / \"splits\"\n",
    "ARTF   = BASE / \"artifacts\"\n",
    "\n",
    "# load classes and checkpoint\n",
    "ckpt = torch.load(ARTF/\"psl_mnv3_best.pth\", map_location=\"cpu\")\n",
    "class_to_idx = ckpt[\"classes\"]\n",
    "idx_to_class = {v:k for k,v in class_to_idx.items()}\n",
    "\n",
    "# dataset reuse from earlier cell\n",
    "test_ds = FileListDataset(SPLITS/\"test.txt\", ARTF/\"classes.json\", train=False)\n",
    "from torch.utils.data import DataLoader\n",
    "test_dl = DataLoader(test_ds, batch_size=64, shuffle=False, num_workers=0, pin_memory=True)\n",
    "\n",
    "# rebuild model exactly as in training\n",
    "num_classes = len(idx_to_class)\n",
    "model_eval = tv.models.mobilenet_v3_small()\n",
    "model_eval.classifier[3] = nn.Linear(model_eval.classifier[3].in_features, num_classes)\n",
    "model_eval.load_state_dict(ckpt[\"state_dict\"]); model_eval.eval()\n",
    "\n",
    "y_true, y_pred = [], []\n",
    "with torch.no_grad():\n",
    "    for xb, yb in test_dl:\n",
    "        logits = model_eval(xb)\n",
    "        y_true.extend(yb.numpy().tolist())\n",
    "        y_pred.extend(logits.argmax(1).numpy().tolist())\n",
    "\n",
    "names = [idx_to_class[i] for i in range(num_classes)]\n",
    "print(classification_report(y_true, y_pred, target_names=names))\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "cm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cd2e1155-5e54-472e-9d9a-cc91dac33569",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\syeds\\AppData\\Local\\Temp\\ipykernel_14220\\3116178430.py:9: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  ckpt = torch.load(ARTF/\"psl_mnv3_best.pth\", map_location=\"cpu\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ONNX: C:\\Users\\syeds\\Documents\\psl-tutor\\artifacts\\psl_mnv3.onnx\n",
      "Classes: C:\\Users\\syeds\\Documents\\psl-tutor\\artifacts\\classes.json\n"
     ]
    }
   ],
   "source": [
    "# I) Export ONNX + save classes.json\n",
    "import torch, torchvision as tv, torch.nn as nn\n",
    "from pathlib import Path\n",
    "import json\n",
    "\n",
    "BASE = Path.cwd().parents[0]\n",
    "ARTF = BASE / \"artifacts\"\n",
    "\n",
    "ckpt = torch.load(ARTF/\"psl_mnv3_best.pth\", map_location=\"cpu\")\n",
    "class_to_idx = ckpt[\"classes\"]\n",
    "num_classes = len(class_to_idx)\n",
    "\n",
    "model_export = tv.models.mobilenet_v3_small()\n",
    "model_export.classifier[3] = nn.Linear(model_export.classifier[3].in_features, num_classes)\n",
    "model_export.load_state_dict(ckpt[\"state_dict\"])\n",
    "model_export.eval()\n",
    "\n",
    "onnx_path = ARTF / \"psl_mnv3.onnx\"\n",
    "dummy = torch.randn(1,3,224,224)\n",
    "torch.onnx.export(model_export, dummy, str(onnx_path),\n",
    "                  input_names=[\"input\"], output_names=[\"logits\"],\n",
    "                  dynamic_axes={\"input\":{0:\"N\"}, \"logits\":{0:\"N\"}},\n",
    "                  opset_version=13)\n",
    "print(\"ONNX:\", onnx_path)\n",
    "\n",
    "# ensure classes map is present alongside ONNX\n",
    "with open(ARTF/\"classes.json\",\"w\") as f:\n",
    "    json.dump(class_to_idx, f, indent=2)\n",
    "print(\"Classes:\", ARTF/\"classes.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0d6b5805-ed70-41aa-9e01-515cbb3f16a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\syeds\\AppData\\Local\\Temp\\ipykernel_14220\\3201539043.py:12: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  ckpt = torch.load(ARTF/\"psl_mnv3_best.pth\", map_location=\"cpu\")\n"
     ]
    }
   ],
   "source": [
    "import cv2, json, torch, torchvision as tv, torch.nn as nn, numpy as np\n",
    "from torchvision import transforms as T\n",
    "from pathlib import Path\n",
    "import mediapipe as mp\n",
    "\n",
    "# -------------------------\n",
    "# LOAD MODEL & LABELS\n",
    "# -------------------------\n",
    "BASE = Path.cwd().parents[0]\n",
    "ARTF = BASE / \"artifacts\"\n",
    "\n",
    "ckpt = torch.load(ARTF/\"psl_mnv3_best.pth\", map_location=\"cpu\")\n",
    "idx_to_class = {v:k for k,v in ckpt[\"classes\"].items()}\n",
    "\n",
    "model = tv.models.mobilenet_v3_small()\n",
    "model.classifier[3] = nn.Linear(model.classifier[3].in_features, len(idx_to_class))\n",
    "model.load_state_dict(ckpt[\"state_dict\"])\n",
    "model.eval()\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model.to(device)\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# NORMALIZATION + PREPROCESS\n",
    "# -------------------------\n",
    "def preprocess(img):\n",
    "    # gamma correction\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    img = img.astype(np.float32) / 255.0\n",
    "    gamma = 1.4\n",
    "    img = np.power(img, gamma)\n",
    "\n",
    "    # back to uint8\n",
    "    img = (img * 255).astype(np.uint8)\n",
    "\n",
    "    # convert to PIL for torchvision\n",
    "    pil = T.ToPILImage()(img)\n",
    "\n",
    "    tf = T.Compose([\n",
    "        T.Resize((224, 224)),\n",
    "        T.ToTensor(),\n",
    "        T.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225])\n",
    "    ])\n",
    "    return tf(pil)\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# MEDIAPIPE HAND DETECTOR\n",
    "# -------------------------\n",
    "mp_hands = mp.solutions.hands.Hands(\n",
    "    static_image_mode=False,\n",
    "    max_num_hands=1,\n",
    "    min_detection_confidence=0.6,\n",
    "    min_tracking_confidence=0.5\n",
    ")\n",
    "\n",
    "def expand_box(xmin, ymin, xmax, ymax, w, h, pad=0.35):\n",
    "    cx = (xmin + xmax) / 2\n",
    "    cy = (ymin + ymax) / 2\n",
    "    side = int(max(xmax - xmin, ymax - ymin) * (1 + pad))\n",
    "    x0 = max(0, int(cx - side // 2))\n",
    "    y0 = max(0, int(cy - side // 2))\n",
    "    x1 = min(w, x0 + side)\n",
    "    y1 = min(h, y0 + side)\n",
    "    return x0, y0, x1, y1\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# PREDICTION SMOOTHING\n",
    "# -------------------------\n",
    "ema_logits = None\n",
    "alpha = 0.6   # smoothing factor\n",
    "cooldown = 0\n",
    "stable_pred = None\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# START CAMERA\n",
    "# -------------------------\n",
    "cap = cv2.VideoCapture(0)\n",
    "assert cap.isOpened(), \"Camera blocked. Enable it in Windows Privacy settings.\"\n",
    "\n",
    "while True:\n",
    "    ok, frame = cap.read()\n",
    "    if not ok: break\n",
    "    h, w = frame.shape[:2]\n",
    "\n",
    "    # detect hand\n",
    "    result = mp_hands.process(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n",
    "\n",
    "    # default ROI center crop fallback\n",
    "    roi = None\n",
    "\n",
    "    if result.multi_hand_landmarks:\n",
    "        lm = result.multi_hand_landmarks[0].landmark\n",
    "        xs = [int(l.x * w) for l in lm]\n",
    "        ys = [int(l.y * h) for l in lm]\n",
    "        x0, y0, x1, y1 = expand_box(min(xs), min(ys), max(xs), max(ys), w, h, pad=0.4)\n",
    "        roi = frame[y0:y1, x0:x1]\n",
    "        cv2.rectangle(frame, (x0,y0), (x1,y1), (0,255,0), 2)\n",
    "    else:\n",
    "        side = min(h, w)\n",
    "        y0 = (h - side) // 2\n",
    "        x0 = (w - side) // 2\n",
    "        roi = frame[y0:y0+side, x0:x0+side]\n",
    "\n",
    "    # preprocess ROI\n",
    "    x = preprocess(roi).unsqueeze(0).to(device)\n",
    "\n",
    "    with torch.no_grad(), torch.autocast(device_type=device, dtype=torch.float16 if device==\"cuda\" else torch.float32):\n",
    "        logits = model(x).float().cpu().numpy()[0]\n",
    "\n",
    "    # EMA smoothing\n",
    "    global_pred = logits\n",
    "    if ema_logits is None:\n",
    "        ema_logits = global_pred\n",
    "    else:\n",
    "        ema_logits = alpha * global_pred + (1 - alpha) * ema_logits\n",
    "\n",
    "    # convert to probability\n",
    "    ex = np.exp(ema_logits - np.max(ema_logits))\n",
    "    probs = ex / ex.sum()\n",
    "\n",
    "    pred_idx = int(probs.argmax())\n",
    "    pred_label = idx_to_class[pred_idx]\n",
    "    conf = float(probs[pred_idx])\n",
    "\n",
    "    # Stabilization: only update final prediction if confidence is high enough\n",
    "    if conf > 0.65:\n",
    "        stable_pred = pred_label\n",
    "        cooldown = 5\n",
    "    elif cooldown > 0:\n",
    "        cooldown -= 1\n",
    "    else:\n",
    "        stable_pred = \"...\"\n",
    "\n",
    "    # draw prediction\n",
    "    cv2.putText(frame, f\"{stable_pred}  {conf:.2f}\", (10, 40),\n",
    "                cv2.FONT_HERSHEY_SIMPLEX, 1.1, (0,255,0), 2)\n",
    "\n",
    "    cv2.imshow(\"PSL CNN + MediaPipe ROI (High Accuracy)\", frame)\n",
    "    if cv2.waitKey(1) == 27:\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "57264cf5-b759-4ae3-9061-651cb2a2e942",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\syeds\\AppData\\Local\\Temp\\ipykernel_14220\\3108989090.py:12: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  ckpt = torch.load(ARTF/\"psl_mnv3_best.pth\", map_location=\"cpu\")\n"
     ]
    }
   ],
   "source": [
    "import cv2, json, torch, torchvision as tv, torch.nn as nn, numpy as np\n",
    "from torchvision import transforms as T\n",
    "from pathlib import Path\n",
    "import mediapipe as mp\n",
    "\n",
    "# -------------------------\n",
    "# LOAD MODEL & LABELS\n",
    "# -------------------------\n",
    "BASE = Path.cwd().parents[0]\n",
    "ARTF = BASE / \"artifacts\"\n",
    "\n",
    "ckpt = torch.load(ARTF/\"psl_mnv3_best.pth\", map_location=\"cpu\")\n",
    "idx_to_class = {v:k for k,v in ckpt[\"classes\"].items()}\n",
    "\n",
    "model = tv.models.mobilenet_v3_small()\n",
    "model.classifier[3] = nn.Linear(model.classifier[3].in_features, len(idx_to_class))\n",
    "model.load_state_dict(ckpt[\"state_dict\"])\n",
    "model.eval()\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model.to(device)\n",
    "\n",
    "# -------------------------\n",
    "# NORMALIZATION + PREPROCESS\n",
    "# -------------------------\n",
    "def preprocess(img):\n",
    "    # gamma correction\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    img = img.astype(np.float32) / 255.0\n",
    "    gamma = 1.4\n",
    "    img = np.power(img, gamma)\n",
    "    img = (img * 255).astype(np.uint8)\n",
    "\n",
    "    pil = T.ToPILImage()(img)\n",
    "    tf = T.Compose([\n",
    "        T.Resize((224, 224)),\n",
    "        T.ToTensor(),\n",
    "        T.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225]),\n",
    "    ])\n",
    "    return tf(pil)\n",
    "\n",
    "# -------------------------\n",
    "# MEDIAPIPE HAND DETECTOR\n",
    "# -------------------------\n",
    "mp_hands = mp.solutions.hands.Hands(\n",
    "    static_image_mode=False,\n",
    "    max_num_hands=1,\n",
    "    min_detection_confidence=0.6,\n",
    "    min_tracking_confidence=0.5\n",
    ")\n",
    "\n",
    "def expand_box(xmin, ymin, xmax, ymax, w, h, pad=0.35):\n",
    "    cx = (xmin + xmax) / 2\n",
    "    cy = (ymin + ymax) / 2\n",
    "    side = int(max(xmax - xmin, ymax - ymin) * (1 + pad))\n",
    "    x0 = max(0, int(cx - side // 2))\n",
    "    y0 = max(0, int(cy - side // 2))\n",
    "    x1 = min(w, x0 + side)\n",
    "    y1 = min(h, y0 + side)\n",
    "    return x0, y0, x1, y1\n",
    "\n",
    "# -------------------------\n",
    "# PREDICTION SMOOTHING\n",
    "# -------------------------\n",
    "ema_logits = None\n",
    "alpha = 0.6   # smoothing factor\n",
    "cooldown = 0\n",
    "stable_pred = None\n",
    "\n",
    "# -------------------------\n",
    "# START CAMERA\n",
    "# -------------------------\n",
    "cap = cv2.VideoCapture(0)\n",
    "assert cap.isOpened(), \"Camera blocked. Enable it in Windows Privacy settings.\"\n",
    "\n",
    "while True:\n",
    "    ok, frame = cap.read()\n",
    "    if not ok: break\n",
    "    h, w = frame.shape[:2]\n",
    "\n",
    "    # detect hand\n",
    "    result = mp_hands.process(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n",
    "\n",
    "    # default ROI center crop fallback\n",
    "    roi = None\n",
    "    hand_label = None  # 'Left' or 'Right' if available\n",
    "\n",
    "    if result.multi_hand_landmarks:\n",
    "        lm = result.multi_hand_landmarks[0].landmark\n",
    "        xs = [int(l.x * w) for l in lm]\n",
    "        ys = [int(l.y * h) for l in lm]\n",
    "        x0, y0, x1, y1 = expand_box(min(xs), min(ys), max(xs), max(ys), w, h, pad=0.4)\n",
    "        roi = frame[y0:y1, x0:x1]\n",
    "        cv2.rectangle(frame, (x0,y0), (x1,y1), (0,255,0), 2)\n",
    "\n",
    "        # handedness\n",
    "        if result.multi_handedness:\n",
    "            hand_label = result.multi_handedness[0].classification[0].label  # 'Left' or 'Right'\n",
    "            # If Left hand, mirror ROI so it matches right-hand training distribution\n",
    "            if hand_label == \"Left\" and roi is not None and roi.size:\n",
    "                roi = cv2.flip(roi, 1)\n",
    "                # Optional: draw label\n",
    "                cv2.putText(frame, \"Left (mirrored)\", (x0, max(0, y0-10)),\n",
    "                            cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0,255,0), 2)\n",
    "            elif hand_label == \"Right\":\n",
    "                cv2.putText(frame, \"Right\", (x0, max(0, y0-10)),\n",
    "                            cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0,255,0), 2)\n",
    "    else:\n",
    "        side = min(h, w)\n",
    "        y0 = (h - side) // 2\n",
    "        x0 = (w - side) // 2\n",
    "        roi = frame[y0:y0+side, x0:x0+side]\n",
    "\n",
    "    # guard against empty ROI\n",
    "    if roi is None or roi.size == 0:\n",
    "        cv2.imshow(\"PSL CNN + MediaPipe ROI (High Accuracy)\", frame)\n",
    "        if cv2.waitKey(1) == 27: break\n",
    "        continue\n",
    "\n",
    "    # preprocess ROI\n",
    "    x = preprocess(roi).unsqueeze(0).to(device)\n",
    "\n",
    "    with torch.no_grad(), torch.autocast(device_type=device, dtype=torch.float16 if device==\"cuda\" else torch.float32):\n",
    "        logits = model(x).float().cpu().numpy()[0]\n",
    "\n",
    "    # EMA smoothing\n",
    "    if ema_logits is None:\n",
    "        ema_logits = logits\n",
    "    else:\n",
    "        ema_logits = alpha * logits + (1 - alpha) * ema_logits\n",
    "\n",
    "    # convert to probability\n",
    "    ex = np.exp(ema_logits - np.max(ema_logits))\n",
    "    probs = ex / ex.sum()\n",
    "\n",
    "    pred_idx = int(probs.argmax())\n",
    "    pred_label = idx_to_class[pred_idx]\n",
    "    conf = float(probs[pred_idx])\n",
    "\n",
    "    # Stabilization: only update final prediction if confidence is high enough\n",
    "    if conf > 0.65:\n",
    "        stable_pred = pred_label\n",
    "        cooldown = 5\n",
    "    elif cooldown > 0:\n",
    "        cooldown -= 1\n",
    "    else:\n",
    "        stable_pred = \"...\"\n",
    "\n",
    "    # draw prediction\n",
    "    txt = f\"{stable_pred}  {conf:.2f}\"\n",
    "    if hand_label: txt += f\"  [{hand_label}]\"\n",
    "    cv2.putText(frame, txt, (10, 40),\n",
    "                cv2.FONT_HERSHEY_SIMPLEX, 1.1, (0,255,0), 2)\n",
    "\n",
    "    cv2.imshow(\"PSL CNN + MediaPipe ROI (High Accuracy)\", frame)\n",
    "    if cv2.waitKey(1) == 27:\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51a05fe4-4c88-42c6-8316-7a8dc3af59f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\syeds\\AppData\\Local\\Temp\\ipykernel_13020\\550973898.py:12: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  ckpt = torch.load(ARTF/\"psl_mnv3_best.pth\", map_location=\"cpu\")\n"
     ]
    }
   ],
   "source": [
    "import cv2, json, torch, torchvision as tv, torch.nn as nn, numpy as np\n",
    "from torchvision import transforms as T\n",
    "from pathlib import Path\n",
    "import mediapipe as mp\n",
    "\n",
    "# -------------------------\n",
    "# LOAD MODEL & LABELS\n",
    "# -------------------------\n",
    "BASE = Path.cwd().parents[0]\n",
    "ARTF = BASE / \"artifacts\"\n",
    "\n",
    "ckpt = torch.load(ARTF/\"psl_mnv3_best.pth\", map_location=\"cpu\")\n",
    "idx_to_class = {v:k for k,v in ckpt[\"classes\"].items()}\n",
    "\n",
    "model = tv.models.mobilenet_v3_small()\n",
    "model.classifier[3] = nn.Linear(model.classifier[3].in_features, len(idx_to_class))\n",
    "model.load_state_dict(ckpt[\"state_dict\"])\n",
    "model.eval()\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model.to(device)\n",
    "\n",
    "# -------------------------\n",
    "# PREPROCESS\n",
    "# -------------------------\n",
    "tf_cam = T.Compose([\n",
    "    T.ToPILImage(),\n",
    "    T.Resize((224, 224)),\n",
    "    T.ToTensor(),\n",
    "    T.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225]),\n",
    "])\n",
    "\n",
    "def preprocess_bgr(img_bgr):\n",
    "    # optional mild gamma for low light; comment out if you prefer raw\n",
    "    img = cv2.cvtColor(img_bgr, cv2.COLOR_BGR2RGB)\n",
    "    img = img.astype(np.float32) / 255.0\n",
    "    img = np.power(img, 1.2)\n",
    "    img = (img * 255).astype(np.uint8)\n",
    "    return tf_cam(img)\n",
    "\n",
    "# -------------------------\n",
    "# MEDIAPIPE HAND DETECTOR\n",
    "# -------------------------\n",
    "mp_hands = mp.solutions.hands.Hands(\n",
    "    static_image_mode=False,\n",
    "    max_num_hands=1,\n",
    "    min_detection_confidence=0.6,\n",
    "    min_tracking_confidence=0.5\n",
    ")\n",
    "\n",
    "def expand_box(xmin, ymin, xmax, ymax, w, h, pad=0.40):\n",
    "    cx = (xmin + xmax) / 2\n",
    "    cy = (ymin + ymax) / 2\n",
    "    side = int(max(xmax - xmin, ymax - ymin) * (1 + pad))\n",
    "    x0 = max(0, int(cx - side // 2))\n",
    "    y0 = max(0, int(cy - side // 2))\n",
    "    x1 = min(w, x0 + side)\n",
    "    y1 = min(h, y0 + side)\n",
    "    return x0, y0, x1, y1\n",
    "\n",
    "# -------------------------\n",
    "# PREDICTION HELPERS\n",
    "# -------------------------\n",
    "@torch.no_grad()\n",
    "def logits_from_img(img_bgr):\n",
    "    x = preprocess_bgr(img_bgr).unsqueeze(0).to(device, non_blocking=True)\n",
    "    with torch.autocast(device_type=device, dtype=torch.float16 if device==\"cuda\" else torch.float32):\n",
    "        out = model(x).float().cpu().numpy()[0]\n",
    "    return out\n",
    "\n",
    "def softmax_conf(logits):\n",
    "    ex = np.exp(logits - logits.max())\n",
    "    probs = ex / ex.sum()\n",
    "    idx = int(probs.argmax())\n",
    "    return probs, idx, float(probs[idx])\n",
    "\n",
    "# EMA smoothing\n",
    "ema_logits = None\n",
    "alpha = 0.6\n",
    "cooldown = 0\n",
    "stable_pred = \"...\"\n",
    "\n",
    "# -------------------------\n",
    "# START CAMERA\n",
    "# -------------------------\n",
    "cap = cv2.VideoCapture(0)\n",
    "assert cap.isOpened(), \"Camera blocked. Enable it in Windows Privacy settings.\"\n",
    "\n",
    "while True:\n",
    "    ok, frame = cap.read()\n",
    "    if not ok: break\n",
    "    h, w = frame.shape[:2]\n",
    "\n",
    "    # detect hand and crop ROI\n",
    "    result = mp_hands.process(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n",
    "    if result.multi_hand_landmarks:\n",
    "        lm = result.multi_hand_landmarks[0].landmark\n",
    "        xs = [int(l.x * w) for l in lm]\n",
    "        ys = [int(l.y * h) for l in lm]\n",
    "        x0,y0,x1,y1 = expand_box(min(xs),min(ys),max(xs),max(ys),w,h,0.40)\n",
    "        roi = frame[y0:y1, x0:x1]\n",
    "        cv2.rectangle(frame, (x0,y0), (x1,y1), (0,255,0), 2)\n",
    "    else:\n",
    "        # fallback: center square\n",
    "        side = min(h, w); y0 = (h - side)//2; x0 = (w - side)//2\n",
    "        roi = frame[y0:y0+side, x0:x0+side]\n",
    "\n",
    "    if roi is None or roi.size == 0:\n",
    "        cv2.imshow(\"PSL CNN + MediaPipe ROI\", frame)\n",
    "        if cv2.waitKey(1) == 27: break\n",
    "        continue\n",
    "\n",
    "    # dual-path inference: original and flipped\n",
    "    logits_orig = logits_from_img(roi)\n",
    "    logits_flip = logits_from_img(cv2.flip(roi, 1))\n",
    "\n",
    "    # choose the path with higher confidence\n",
    "    probs_o, idx_o, conf_o = softmax_conf(logits_orig)\n",
    "    probs_f, idx_f, conf_f = softmax_conf(logits_flip)\n",
    "\n",
    "    if conf_f > conf_o:\n",
    "        chosen_logits = logits_flip\n",
    "        pred_idx = idx_f\n",
    "        conf = conf_f\n",
    "        used = \"flip\"\n",
    "    else:\n",
    "        chosen_logits = logits_orig\n",
    "        pred_idx = idx_o\n",
    "        conf = conf_o\n",
    "        used = \"orig\"\n",
    "\n",
    "    # EMA smoothing over chosen logits\n",
    "    if ema_logits is None:\n",
    "        ema_logits = chosen_logits\n",
    "    else:\n",
    "        ema_logits = alpha * chosen_logits + (1 - alpha) * ema_logits\n",
    "\n",
    "    # final probability after smoothing\n",
    "    probs, idx_final, conf_final = softmax_conf(ema_logits)\n",
    "    pred_label = idx_to_class[idx_final]\n",
    "\n",
    "    # simple stability gate\n",
    "    if conf_final > 0.65:\n",
    "        stable_pred = pred_label\n",
    "        cooldown = 5\n",
    "    elif cooldown > 0:\n",
    "        cooldown -= 1\n",
    "    else:\n",
    "        stable_pred = \"...\"\n",
    "\n",
    "    # draw\n",
    "    cv2.putText(frame, f\"{stable_pred} {conf_final:.2f} [{used}]\",\n",
    "                (10, 40), cv2.FONT_HERSHEY_SIMPLEX, 1.1, (0,255,0), 2)\n",
    "\n",
    "    cv2.imshow(\"PSL CNN + MediaPipe ROI\", frame)\n",
    "    if cv2.waitKey(1) == 27: break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4974a62e-61d8-4f75-8b14-117ca7ad2969",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (psl-cnn)",
   "language": "python",
   "name": "psl-cnn"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
